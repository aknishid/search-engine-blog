{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "908cf362670d2f71",
   "metadata": {},
   "source": [
    "# SIFTデータセットを使ったパフォーマンス検証\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51cf6915be9b2c33",
   "metadata": {},
   "source": [
    "## データの準備\n",
    "[SIFT](https://github.com/erikbern/ann-benchmarks/tree/main?tab=readme-ov-file)ベクトルデータセットを利用します。\n",
    "近傍100件のIDがデータセットに含まれているので、検索結果の精度を測定できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca8e1dbb38709cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget http://ann-benchmarks.com/sift-128-euclidean.hdf5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5c702c5cac5270ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distances\n",
      "neighbors\n",
      "test\n",
      "train\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import pandas as pd\n",
    "dataset_path = 'sift-128-euclidean.hdf5'\n",
    "\n",
    "with h5py.File(dataset_path, 'r') as f:\n",
    "    def print_hdf5_structure(name, obj):\n",
    "        print(name)\n",
    "    f.visititems(print_hdf5_structure)\n",
    "    distances = f['distances'][:]\n",
    "    neighbors = f['neighbors'][:]\n",
    "    train = f['train'][:]\n",
    "    test = f['test'][:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "213cd77f9e630572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# データフレームに変換し、IDを生成\n",
    "train_df = pd.DataFrame({'vector': [list(vec) for vec in train]})\n",
    "train_df['id'] = [f\"{i}\" for i in range(len(train_df))]\n",
    "\n",
    "test_df = pd.DataFrame({'vector': [list(vec) for vec in test]})\n",
    "test_df['id'] = [f\"img{i}\" for i in range(len(test_df))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6edbf79c-c51f-4460-9d23-6a21aa77572a",
   "metadata": {},
   "outputs": [],
   "source": [
    "distances_df = pd.DataFrame(distances)\n",
    "neighbors_df = pd.DataFrame(neighbors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c47dd7398abde6b",
   "metadata": {},
   "source": [
    "## 精度の計算\n",
    "[Vertex AI ベクトル検索](https://cloud.google.com/vertex-ai/docs/vector-search/overview?hl=ja)で紹介されているRecallを精度指標とします。\n",
    "\n",
    "このRecallは、検索結果の中で実際に正しい近傍データであるものの割合です。\n",
    "\n",
    "例えば、10個の検索結果を得るクエリを実行して9個の正解の最近傍を返した場合に、Recallは`9/10 = 0.9`になります。\n",
    "\n",
    "$$\n",
    "\\text{Recall} = \\frac{\\text{Number of relevant documents retrieved in top } k}{k}\n",
    "$$\n",
    "\n",
    "このリコールの値は0から1の範囲をとり、1に近いほど、関連するドキュメントを検索できていることを示します。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ac61d01c-4658-4dc4-aced-b34696ddda76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_recall_byid(all_result_ids, k=10):\n",
    "    recalls = []\n",
    "    num_samples = len(all_result_ids.keys())\n",
    "    for n in range(num_samples):\n",
    "        true_ids = set(int(res) for res in neighbors_df.iloc[n].tolist()[:k])\n",
    "        retrieved_ids = set(all_result_ids[n][:k])\n",
    "        \n",
    "        # 正しいIDのセットと取得したIDのセットの交差を使ってリコールを計算\n",
    "        cnt = len(true_ids.intersection(retrieved_ids))\n",
    "        recall = cnt / float(k)\n",
    "        recalls.append(recall)\n",
    "        # print(f\"Recall for test instance {n}: {recall:.4f}\")\n",
    "\n",
    "    average_recall = np.mean(recalls)\n",
    "    print(f\"Average Recall over {num_samples} instances: {average_recall:.4f}\")\n",
    "    return average_recall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096b674b-6447-415d-bd95-c0cda1b8c339",
   "metadata": {},
   "source": [
    "## 速度の計測\n",
    "今回は簡易的な検証のため、専用のロードテストツールではなくJupyter Notebook上で以下のように検証を行います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "57e854c1-f310-409a-9e8d-d13c9913c9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "def perform_search(test_df, search_function, k=10, num_iterations=1, timeout=None):\n",
    "    percentile_90_list = []\n",
    "    percentile_99_list = []\n",
    "    total_execution_times = []\n",
    "    all_result_ids = {}\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "        print(f\"Iteration {i+1}\")\n",
    "\n",
    "        search_times = []\n",
    "\n",
    "        for index, row in tqdm(test_df.iterrows(), desc=\"Searching\", total=test_df.shape[0]):\n",
    "            vector = row['vector']\n",
    "            start_time = time.time()\n",
    "            try:\n",
    "                response = search_function(vector, timeout)\n",
    "            except Exception as e:\n",
    "                print(f\"Timeout or error occurred for index {index}: {e}\")\n",
    "                continue\n",
    "\n",
    "            end_time = time.time()\n",
    "            search_times.append(end_time - start_time)\n",
    "\n",
    "            result_ids = [int(res_id) for res_id in response[:k]]  # 上位k件のドキュメントIDを取得\n",
    "            all_result_ids[index] = result_ids\n",
    "\n",
    "        df = pd.DataFrame(search_times, columns=['search_time'])\n",
    "\n",
    "        percentile_90 = df['search_time'].quantile(0.9) * 1000  # ミリ秒に変換\n",
    "        percentile_99 = df['search_time'].quantile(0.99) * 1000  # ミリ秒に変換\n",
    "\n",
    "        percentile_90_list.append(percentile_90)\n",
    "        percentile_99_list.append(percentile_99)\n",
    "        total_execution_times.append(sum(search_times))\n",
    "\n",
    "        print(\"Total execution time for searches: {:.3f} s\".format(sum(search_times)))\n",
    "        print(\"90th percentile of search times: {:.3f} ms\".format(percentile_90))\n",
    "        print(\"99th percentile of search times: {:.3f} ms\".format(percentile_99))\n",
    "\n",
    "    average_90 = np.mean(percentile_90_list)\n",
    "    std_90 = np.std(percentile_90_list)\n",
    "\n",
    "    average_99 = np.mean(percentile_99_list)\n",
    "    std_99 = np.std(percentile_99_list)\n",
    "\n",
    "    average_total_time = np.mean(total_execution_times)\n",
    "    std_total_time = np.std(total_execution_times)\n",
    "\n",
    "    print(\"Average total execution time: {:.3f} s (std: {:.3f})\".format(average_total_time, std_total_time))\n",
    "    print(\"Average 90th percentile of search times: {:.3f} ms (std: {:.3f})\".format(average_90, std_90))\n",
    "    print(\"Average 99th percentile of search times: {:.3f} ms (std: {:.3f})\".format(average_99, std_99))\n",
    "\n",
    "    return all_result_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e513cc",
   "metadata": {},
   "source": [
    "# Vald"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "52e3c830",
   "metadata": {},
   "outputs": [],
   "source": [
    "import grpc\n",
    "from vald.v1.vald import insert_pb2_grpc\n",
    "from vald.v1.vald import upsert_pb2_grpc\n",
    "from vald.v1.vald import search_pb2_grpc\n",
    "from vald.v1.vald import update_pb2_grpc\n",
    "from vald.v1.vald import remove_pb2_grpc\n",
    "from vald.v1.vald import object_pb2_grpc\n",
    "from vald.v1.vald import index_pb2_grpc\n",
    "from vald.v1.payload import payload_pb2\n",
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "33a0b6f6-d47e-4398-be3c-0758c9e3ec97",
   "metadata": {},
   "outputs": [],
   "source": [
    "PORT_DEFAULT = ':8081'\n",
    "host = \"vald-lb-gateway.test-ns.svc.cluster.local\"\n",
    "\n",
    "options = [\n",
    "    ('grpc.max_metadata_size', 32 * 1024),\n",
    "]\n",
    "\n",
    "## create a channel by passing \"{host}:{port}\"\n",
    "channel = grpc.insecure_channel(host + PORT_DEFAULT, options=options)\n",
    "\n",
    "## create stubs for calling RPCs\n",
    "insertStub = insert_pb2_grpc.InsertStub(channel)\n",
    "upsertStub = upsert_pb2_grpc.UpsertStub(channel)\n",
    "updateStub = update_pb2_grpc.UpdateStub(channel)\n",
    "removeStub = remove_pb2_grpc.RemoveStub(channel)\n",
    "objectStub = object_pb2_grpc.ObjectStub(channel)\n",
    "searchStub = search_pb2_grpc.SearchStub(channel)\n",
    "indexStub = index_pb2_grpc.IndexStub(channel)\n",
    "\n",
    "insertConfig = payload_pb2.Insert.Config(skip_strict_exist_check=True)\n",
    "updateConfig = payload_pb2.Update.Config(skip_strict_exist_check=True)\n",
    "removeConfig = payload_pb2.Remove.Config(skip_strict_exist_check=True)\n",
    "upsertConfig = payload_pb2.Upsert.Config(skip_strict_exist_check=True)\n",
    "searchConfig = payload_pb2.Search.Config(num=10, radius=-1, epsilon=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddaafbee-47b7-4bd4-b227-7c16191c6148",
   "metadata": {},
   "source": [
    "## Insert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1d3f25b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing Upsert Data: 100%|██████████| 1000000/1000000 [01:11<00:00, 14056.39it/s]\n"
     ]
    }
   ],
   "source": [
    "def generatorUpsertStream(df):\n",
    "    upsert_list = []\n",
    "    for _, row in tqdm(df.iterrows(), total=df.shape[0], desc=\"Preparing Upsert Data\"):\n",
    "        v = payload_pb2.Object.Vector(id=row['id'], vector=row['vector'])\n",
    "        upsert_list.append(payload_pb2.Upsert.Request(vector=v, config=upsertConfig))\n",
    "    return upsert_list\n",
    "\n",
    "def chunked_iterable(iterable, chunk_size):\n",
    "    \"\"\"Generates chunks of data from an iterable.\"\"\"\n",
    "    for i in range(0, len(iterable), chunk_size):\n",
    "        yield iterable[i:i + chunk_size]\n",
    "\n",
    "vec = generatorUpsertStream(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bd0fad36-45f6-4de3-868f-977d59c6a60f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Upsert Data: 100%|██████████| 50/50 [05:42<00:00,  6.86s/it]\n"
     ]
    }
   ],
   "source": [
    "chunk_size = 20000\n",
    "\n",
    "# vecをチャンクに分割して処理\n",
    "for chunk in tqdm(chunked_iterable(vec, chunk_size), total=(len(vec) + chunk_size - 1) // chunk_size, desc=\"Upsert Data\"):\n",
    "    for _ in upsertStub.StreamUpsert(iter(chunk)):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60fd4afacaec063",
   "metadata": {},
   "source": [
    "### Indexing完了確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bb7775e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current index count: 3000000\n",
      "Indexing completed in: 0.01 seconds\n"
     ]
    }
   ],
   "source": [
    "index_size = 1000000\n",
    "check_interval = 5 # チェック間隔（秒）\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "while True:\n",
    "    res = indexStub.IndexInfo(payload_pb2.Empty())\n",
    "    current_count = res.stored\n",
    "\n",
    "    print(f\"Current index count: {current_count}\")\n",
    "\n",
    "    if current_count >= index_size * 3: # index_replica=3\n",
    "        end_time = time.time()\n",
    "        print(\"Indexing completed in: {:.2f} seconds\".format(end_time - start_time))\n",
    "        break\n",
    "\n",
    "    time.sleep(check_interval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd84bf7e-682d-4b55-93ea-bb3a0fd19272",
   "metadata": {},
   "source": [
    "## Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b808a640-934b-49cc-b44e-93669edcf77e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Searching: 100%|██████████| 10000/10000 [00:48<00:00, 207.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total execution time for searches: 46.998 s\n",
      "90th percentile of search times: 8.540 ms\n",
      "99th percentile of search times: 15.398 ms\n",
      "Average total execution time: 46.998 s (std: 0.000)\n",
      "Average 90th percentile of search times: 8.540 ms (std: 0.000)\n",
      "Average 99th percentile of search times: 15.398 ms (std: 0.000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def vald_search_function(vector, _):\n",
    "    response = searchStub.Search(payload_pb2.Search.Request(vector=vector, config=searchConfig))\n",
    "    return [int(result.id) for result in response.results]\n",
    "\n",
    "# Vald\n",
    "vald_all_result_ids = perform_search(test_df, vald_search_function, k=10, num_iterations=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "62dee469-6897-4daa-9f04-68095734ca2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Recall over 10000 instances: 0.9993\n"
     ]
    }
   ],
   "source": [
    "# Vald\n",
    "average_recall = calculate_recall_byid(vald_all_result_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a98b2be",
   "metadata": {},
   "source": [
    "# Opensearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8cd20d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from opensearchpy import OpenSearch\n",
    "from opensearchpy.exceptions import ConnectionError\n",
    "import time\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5a142369",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib3\n",
    "# InsecureRequestWarningを無効化\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ce4f039d",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_admin_password = '' # set your password"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b71b055a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train_dfのvectorの次元数を取得\n",
    "vector_dim = len(train_df['vector'][0])\n",
    "vector_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8e55b1ef-ff1a-400c-98b7-ab405706b1f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/opensearchpy/connection/http_urllib3.py:214: UserWarning: Connecting to https://my-third-cluster.opensearch-3.svc.cluster.local:9200 using SSL with verify_certs=False is insecure.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "client = OpenSearch(\n",
    "    hosts=[{'host': 'my-third-cluster.opensearch-3.svc.cluster.local', 'port': 9200}],\n",
    "    http_auth=('admin', initial_admin_password),\n",
    "    use_ssl=True,\n",
    "    verify_certs=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2013ab3f-9db6-44ae-bc45-d7ad37909b80",
   "metadata": {},
   "source": [
    "## Insert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "93723ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_documents_bulk(df, index_name, bulk_size, max_retries=3, retry_delay=5):\n",
    "    bulk_data = []\n",
    "\n",
    "    for _, row in tqdm(df.iterrows(), total=df.shape[0], desc=\"Indexing Documents\"):\n",
    "        action = {\n",
    "            \"index\": {\n",
    "                \"_index\": index_name,\n",
    "                \"_id\": row['id']\n",
    "            }\n",
    "        }\n",
    "        vector = [float(v) for v in row['vector']]\n",
    "\n",
    "        document = {\n",
    "            'vec_id': row['id'],\n",
    "            'vector': vector\n",
    "        }\n",
    "\n",
    "        bulk_data.append(json.dumps(action))\n",
    "        bulk_data.append(json.dumps(document))\n",
    "\n",
    "        if len(bulk_data) >= 2 * bulk_size:\n",
    "            success = False\n",
    "            retries = 0\n",
    "            while not success and retries < max_retries:\n",
    "                try:\n",
    "                    client.bulk(body=\"\\n\".join(bulk_data) + \"\\n\")\n",
    "                    success = True\n",
    "                except Exception as e:\n",
    "                    print(f\"Error indexing documents: {e}\")\n",
    "                    retries += 1\n",
    "                    if retries < max_retries:\n",
    "                        print(f\"Retrying... ({retries}/{max_retries})\")\n",
    "                        time.sleep(retry_delay)\n",
    "                    else:\n",
    "                        print(\"Max retries reached. Skipping this batch.\")\n",
    "            bulk_data = []  # バッファをクリア\n",
    "\n",
    "    # 残りのデータのリクエスト\n",
    "    if bulk_data:\n",
    "        success = False\n",
    "        retries = 0\n",
    "        while not success and retries < max_retries:\n",
    "            try:\n",
    "                client.bulk(body=\"\\n\".join(bulk_data) + \"\\n\")\n",
    "                success = True\n",
    "            except Exception as e:\n",
    "                print(f\"Error indexing documents: {e}\")\n",
    "                retries += 1\n",
    "                if retries < max_retries:\n",
    "                    print(f\"Retrying... ({retries}/{max_retries})\")\n",
    "                    time.sleep(retry_delay)\n",
    "                else:\n",
    "                    print(\"Max retries reached. Skipping final batch.\")\n",
    "\n",
    "    print(\"All documents have been indexed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b8ce5e1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Indexing Documents: 100%|██████████| 1000000/1000000 [03:35<00:00, 4629.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All documents have been indexed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "index_name = 'sift-nmslib-l2-24' # indexは事前に作成しておく\n",
    "index_documents_bulk(train_df, index_name, 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05dcdad3-f6fe-42bb-951c-67a0555c62c7",
   "metadata": {},
   "source": [
    "### Indexing完了確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "589d353d-bf60-4347-9f61-1d9cb5bee20c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current document count: 1000000\n",
      "Indexing completed in 0.00 seconds.\n"
     ]
    }
   ],
   "source": [
    "index_count = 1000000\n",
    "check_interval = 5 # チェック間隔（秒）\n",
    "\n",
    "def get_document_count(index_name):\n",
    "    try:\n",
    "        response = client.indices.stats(index=index_name)\n",
    "        doc_count = response['_all']['primaries']['docs']['count']\n",
    "        return doc_count\n",
    "    except ConnectionError as e:\n",
    "        print(f\"Connection error: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "while True:\n",
    "    doc_count = get_document_count(index_name)\n",
    "    \n",
    "    if doc_count is not None:\n",
    "        print(f\"Current document count: {doc_count}\")\n",
    "\n",
    "        if doc_count >= index_count:\n",
    "            end_time = time.time()\n",
    "            elapsed_time = end_time - start_time\n",
    "            print(f\"Indexing completed in {elapsed_time:.2f} seconds.\")\n",
    "            break\n",
    "\n",
    "    time.sleep(check_interval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13ebcc2",
   "metadata": {},
   "source": [
    "## Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b180a42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(vector, index_name, timeout):\n",
    "    search_query = {\n",
    "    \"size\": 10,\n",
    "        \"query\": {\n",
    "            \"knn\": {\n",
    "            \"vector\": {\n",
    "                \"vector\": vector,\n",
    "                \"k\": 10,\n",
    "                \"method_parameters\" : {\n",
    "                  \"ef_search\": 100\n",
    "                }\n",
    "            }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    response = client.search(\n",
    "        index=index_name,\n",
    "        body=search_query,\n",
    "        params={},\n",
    "        request_timeout=timeout\n",
    "    )\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b8327e2b-d04c-424c-9139-89ac69f2ddbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Searching: 100%|██████████| 10000/10000 [02:56<00:00, 56.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total execution time for searches: 173.882 s\n",
      "90th percentile of search times: 19.394 ms\n",
      "99th percentile of search times: 24.289 ms\n",
      "Average total execution time: 173.882 s (std: 0.000)\n",
      "Average 90th percentile of search times: 19.394 ms (std: 0.000)\n",
      "Average 99th percentile of search times: 24.289 ms (std: 0.000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def open_search_function(vector, timeout):\n",
    "    response = search(vector, index_name, timeout)\n",
    "    return [int(res['_id']) for res in response['hits']['hits']]\n",
    "    \n",
    "opensearch_all_result_ids = perform_search(test_df, open_search_function, k=10, num_iterations=1, timeout=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "480c81c8-23b5-436a-8886-68aaf794eb8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Recall over 10000 instances: 0.9989\n"
     ]
    }
   ],
   "source": [
    "average_recall = calculate_recall_byid(opensearch_all_result_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60967ba-f09e-4bdc-b1be-bc3929f40229",
   "metadata": {},
   "source": [
    "## 謝辞\n",
    "データセットは以下ライセンスに基づき使用させていただきました。\n",
    "\n",
    "http://corpus-texmex.irisa.fr/\n",
    "\n",
    "データセットを公開いただきましたLaurent Amsaleg様とHervé Jégou様、ANN Benchmarksを公開いただきましたErik Bernhardsson様に感謝を申し上げます。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
